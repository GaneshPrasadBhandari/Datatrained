{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"\n", "Created on Tue Feb 26 16:00:07 2021"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@author: Ganesh Prasad\n", "\"\"\"\n", "# Load Libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import numpy as np\n", "import pandas as pd\n", "from matplotlib import pyplot\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.svm import SVC\n", "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "#import graphviz"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"D:/datatrained_project/practice projects/mashroom/mushrooms.csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Have a look into data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Have a look into records\", df.head(10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["column and data type"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Column and Data Type\", df.info())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["shape of data type"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Shape of Dataset\", df.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["stats of dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Describe the Data \", df.describe())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["validate class is having only two attribute"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Unique value in Class \", df[\"class\"].unique())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Class distribution for e and p"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Count of the class for e and p\", df['class'].value_counts())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nan_in_df = df.isnull().values.sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print the dataframe for NAN"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Null value in input column\",nan_in_df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Null in feature column\",df['class'].isnull().values.sum())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for column in df.columns:\n", "    df[column] = pd.get_dummies(df[column])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n data after one hot encoding\",df.head(10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["class distribution"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(df.groupby('class').size())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unimodal Data Visualizations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["histograms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Multimodal Data Visualizations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["correlation matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(14,12))\n", "sns.heatmap(df.corr(),linewidths=.1,cmap=\"Purples\", annot=True, annot_kws={\"size\": 7})\n", "plt.yticks(rotation=0)\n", "plt.savefig(\"corr.png\", format='png', dpi=900, bbox_inches='tight')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Validation Dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split-out validation dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = df.values\n", "X = array[:,1:22]\n", "Y = array[:,0]\n", "validation_size = 0.20\n", "seed = 7\n", "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,test_size=validation_size, random_state=seed)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test options and evaluation metric"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_folds = 10\n", "seed = 7\n", "scoring = 'accuracy'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Spot-Check Algorithms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = []\n", "models.append(('LR', LogisticRegression()))\n", "models.append(('LDA', LinearDiscriminantAnalysis()))\n", "models.append(('KNN', KNeighborsClassifier()))\n", "models.append(('CART', DecisionTreeClassifier()))\n", "models.append(('NB', GaussianNB()))\n", "models.append(('SVM', SVC()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = []\n", "names = []\n", "for name, model in models:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)\n", "# Output\n", "# LR: 0.949684 (0.005679)\n", "# LDA: 0.939374 (0.007613)\n", "# KNN: 0.973072 (0.006961)\n", "# CART: 0.979228 (0.003844)\n", "# NB: 0.893216 (0.016749)\n", "# SVM: 0.978920 (0.004460)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compare Algorithms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = pyplot.figure()\n", "fig.suptitle('Algorithm Comparison')\n", "ax = fig.add_subplot(111)\n", "pyplot.boxplot(results)\n", "ax.set_xticklabels(names)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate Algorithms: Standardize/Normalize data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines<br>\n", "that standardize the data and build the # model for each fold in the cross-validation test harness.<br>\n", "That way we can get a fair estimation of how each model with standardized data might perform on unseen data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Standardize the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pipelines = []\n", "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n", "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n", "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n", "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n", "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n", "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n", "results = []\n", "names = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for name, model in pipelines:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)\n", "##Output\n", "# ScaledLR: 0.953223 (0.006505)\n", "# ScaledLDA: 0.939374 (0.007613)\n", "# ScaledKNN: 0.976611 (0.005237)\n", "# ScaledCART: 0.979228 (0.003844)\n", "# ScaledNB: 0.910600 (0.006561)\n", "# ScaledSVM: 0.978766 (0.004228)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compare Algorithms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = pyplot.figure()\n", "fig.suptitle('Scaled Algorithm Comparison')\n", "ax = fig.add_subplot(111)\n", "pyplot.boxplot(results)\n", "ax.set_xticklabels(names)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After investigate tuning the parameters for two algorithms that show promise from<br>\n", "In the spot-checking highest score is for : KNN and SVM.<br>\n", "Output is<br>\n", "ScaledLR: 0.953223 (0.006505)<br>\n", "ScaledLDA: 0.939374 (0.007613)<br>\n", "ScaledKNN: 0.976611 (0.005237)<br>\n", "ScaledCART: 0.979228 (0.003844)<br>\n", "ScaledNB: 0.910600 (0.006561)<br>\n", "ScaledSVM: 0.978766 (0.004228)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tuning KNN"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tune scaled KNN"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler().fit(X_train)\n", "rescaledX = scaler.transform(X_train)\n", "neighbors = [1,3,5,7,9,11,13,15,17,19,21]\n", "param_grid = dict(n_neighbors=neighbors)\n", "model = KNeighborsClassifier()\n", "kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n", "grid_result = grid.fit(rescaledX, Y_train)\n", "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "means = grid_result.cv_results_['mean_test_score']\n", "stds = grid_result.cv_results_['std_test_score']\n", "params = grid_result.cv_results_['params']\n", "for mean, stdev, param in zip(means, stds, params):\n", "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["utput after tunning of KNN<br>\n", "Best: 0.979074 using {'n_neighbors': 1}<br>\n", "0.979074 (0.004468) with: {'n_neighbors': 1}<br>\n", "0.973533 (0.006632) with: {'n_neighbors': 3}<br>\n", "0.977688 (0.005348) with: {'n_neighbors': 5}<br>\n", "0.978613 (0.004319) with: {'n_neighbors': 7}<br>\n", "0.977843 (0.004361) with: {'n_neighbors': 9}<br>\n", "0.976458 (0.004718) with: {'n_neighbors': 11}<br>\n", "0.976920 (0.005102) with: {'n_neighbors': 13}<br>\n", "0.976920 (0.005102) with: {'n_qneighbors': 15}<br>\n", "0.976920 (0.005102) with: {'n_neighbors': 17}<br>\n", "0.976920 (0.005102) with: {'n_neighbors': 19}<br>\n", "0.976920 (0.005102) with: {'n_neighbors': 21}<br>\n", "Best: 0.979689 using {'C': 1.7, 'kernel': 'rbf'}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tuning SVM<br>\n", "tune two key parameters of the SVM algorithm, the value of C (how much to relax the<br>\n", "margin) and the type of kernel. The default for SVM (the SVC class) is to use the Radial<br>\n", "Basis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid<br>\n", "search using 10-fold cross-validation with a standardized copy of the training dataset. We will<br>\n", "try a number of simpler kernel types and C values with less bias and more bias (less than and<br>\n", "more than 1.0 respectively)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["une scaled SVM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler().fit(X_train)\n", "rescaledX = scaler.transform(X_train)\n", "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n", "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n", "param_grid = dict(C=c_values, kernel=kernel_values)\n", "model = SVC()\n", "kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n", "grid_result = grid.fit(rescaledX, Y_train)\n", "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "means = grid_result.cv_results_['mean_test_score']\n", "stds = grid_result.cv_results_['std_test_score']\n", "params = grid_result.cv_results_['params']\n", "for mean, stdev, param in zip(means, stds, params):\n", "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["0.958147 (0.005995) with: {'C': 0.1, 'kernel': 'linear'}<br>\n", "0.938297 (0.010319) with: {'C': 0.1, 'kernel': 'poly'}<br>\n", "0.972610 (0.007311) with: {'C': 0.1, 'kernel': 'rbf'}<br>\n", "0.913678 (0.005089) with: {'C': 0.1, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 0.3, 'kernel': 'linear'}<br>\n", "0.973072 (0.007195) with: {'C': 0.3, 'kernel': 'poly'}<br>\n", "0.972918 (0.007226) with: {'C': 0.3, 'kernel': 'rbf'}<br>\n", "0.897522 (0.009191) with: {'C': 0.3, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 0.5, 'kernel': 'linear'}<br>\n", "0.973995 (0.005739) with: {'C': 0.5, 'kernel': 'poly'}<br>\n", "0.974303 (0.005723) with: {'C': 0.5, 'kernel': 'rbf'}<br>\n", "0.897059 (0.007856) with: {'C': 0.5, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 0.7, 'kernel': 'linear'}<br>\n", "0.978766 (0.004228) with: {'C': 0.7, 'kernel': 'poly'}<br>\n", "0.978920 (0.004406) with: {'C': 0.7, 'kernel': 'rbf'}<br>\n", "0.897521 (0.008708) with: {'C': 0.7, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 0.9, 'kernel': 'linear'}<br>\n", "0.978613 (0.004264) with: {'C': 0.9, 'kernel': 'poly'}<br>\n", "0.978766 (0.004228) with: {'C': 0.9, 'kernel': 'rbf'}<br>\n", "0.897828 (0.008950) with: {'C': 0.9, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 1.0, 'kernel': 'linear'}<br>\n", "0.978613 (0.004264) with: {'C': 1.0, 'kernel': 'poly'}<br>\n", "0.978766 (0.004228) with: {'C': 1.0, 'kernel': 'rbf'}<br>\n", "0.897675 (0.008790) with: {'C': 1.0, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 1.3, 'kernel': 'linear'}<br>\n", "0.979228 (0.004198) with: {'C': 1.3, 'kernel': 'poly'}<br>\n", "0.978920 (0.004242) with: {'C': 1.3, 'kernel': 'rbf'}<br>\n", "0.897675 (0.008461) with: {'C': 1.3, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 1.5, 'kernel': 'linear'}<br>\n", "0.979228 (0.004198) with: {'C': 1.5, 'kernel': 'poly'}<br>\n", "0.978920 (0.004242) with: {'C': 1.5, 'kernel': 'rbf'}<br>\n", "0.897368 (0.006993) with: {'C': 1.5, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 1.7, 'kernel': 'linear'}<br>\n", "0.979228 (0.004198) with: {'C': 1.7, 'kernel': 'poly'}<br>\n", "0.979689 (0.003626) with: {'C': 1.7, 'kernel': 'rbf'}<br>\n", "0.897368 (0.006993) with: {'C': 1.7, 'kernel': 'sigmoid'}<br>\n", "0.959224 (0.006860) with: {'C': 2.0, 'kernel': 'linear'}<br>\n", "0.979536 (0.003770) with: {'C': 2.0, 'kernel': 'poly'}<br>\n", "0.979689 (0.003626) with: {'C': 2.0, 'kernel': 'rbf'}<br>\n", "0.897059 (0.008438) with: {'C': 2.0, 'kernel': 'sigmoid'}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#########Ensemble Methods###########"]}, {"cell_type": "markdown", "metadata": {}, "source": ["evaluate four different ensemble machine learning<br>\n", "algorithms, two boosting and two bagging methods:<br>\n", "1. Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).<br>\n", "2. Bagging Methods: Random Forests (RF) and Extra Trees (ET)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["ensembles"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ensembles = []\n", "ensembles.append(('AB', AdaBoostClassifier()))\n", "ensembles.append(('GBM', GradientBoostingClassifier()))\n", "ensembles.append(('RF', RandomForestClassifier()))\n", "ensembles.append(('ET', ExtraTreesClassifier()))\n", "results = []\n", "names = []\n", "for name, model in ensembles:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)\n", "# Output is\n", "#AB: 0.940913 (0.005893)\n", "#GBM: 0.967533 (0.007102)\n", "#RF: 0.979536 (0.003510)\n", "#ET: 0.979382 (0.003716)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finalize model<br>\n", "SVM showed the most promise as a low complexity and stable model for this problem. In<br>\n", "so Finalize the model by training it on the entire training dataset and make<br>\n", "predictions for the hold-out validation dataset to confirm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["prepare the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler().fit(X_train)\n", "rescaledX = scaler.transform(X_train)\n", "model = SVC(C=1.5)\n", "model.fit(rescaledX, Y_train)\n", "# estimate accuracy on validation dataset\n", "rescaledValidationX = scaler.transform(X_validation)\n", "predictions = model.predict(rescaledValidationX)\n", "print(accuracy_score(Y_validation, predictions))\n", "print(confusion_matrix(Y_validation, predictions))\n", "print(classification_report(Y_validation, predictions))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#Output is"]}, {"cell_type": "markdown", "metadata": {}, "source": ["0.9796923076923076<br>\n", "[[754  31]<br>\n", " [  2 838]]<br>\n", "              precision    recall  f1-score   support<br>\n", "<br>\n", "           0       1.00      0.96      0.98       785<br>\n", "           1       0.96      1.00      0.98       840<br>\n", "<br>\n", "    accuracy                           0.98      1625<br>\n", "   macro avg       0.98      0.98      0.98      1625<br>\n", "weighted avg       0.98      0.98      0.98      1625"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}