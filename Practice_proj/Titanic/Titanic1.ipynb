{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Created on Fri Feb 26 13:48:10 2021<br>\n", "@author: Ganesh<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the Dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load libraries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from matplotlib import pyplot\n", "import matplotlib.pyplot as plt\n", "from pandas import read_csv\n", "from pandas import set_option\n", "import pandas as pd\n", "from sklearn.preprocessing import LabelEncoder\n", "from keras.utils import np_utils\n", "from sklearn import preprocessing\n", "from sklearn.preprocessing import OneHotEncoder\n", "from sklearn.preprocessing import MultiLabelBinarizer\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.impute import SimpleImputer\n", "from pandas.plotting import scatter_matrix\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import classification_report\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.svm import SVC\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.ensemble import ExtraTreesClassifier\n", "import seaborn as sns\n", "#df = pd.read_csv('D:/datatrained_project/titanic/titanic_train.csv')\n", "df1 = pd.read_csv('D:/datatrained_project/practice projects/titanic/titanic_train.csv')\n", "df = pd.DataFrame(df1)\n", "print(\"\\n Have a look into records\", df.head(10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["column and data type"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Column and Data Type\", df.info())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["shape of data type"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Shape of Dataset\", df.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["stats of dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Describe the Data \", df.describe())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Survived class is having only two attribute"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Unique value in Class \", df[\"Survived\"].unique())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Class distribution for not survived and survived"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Count of the class for not survived and survived\", df['Survived'].value_counts())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.countplot(x='Survived',data = df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.countplot(x='Survived', hue = 'Sex', data = df)\n", "df.info\n", "sns.heatmap(df.isnull(), yticklabels = False, cmap ='viridis' )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.boxplot(x='Pclass', y= 'Age', data = df)\n", "df.head(5)\n", "df.drop(['Cabin','Name','Ticket'], axis = 1, inplace = True)\n", "print(\"Nan value in df\",df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.isnull().sum()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.dropna(inplace = True)\n", "df.isnull().sum()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sex = pd.get_dummies(df['Sex'] , drop_first = True)\n", "print(sex.head(5))\n", "df.columns\n", "embarked = pd.get_dummies(df['Embarked'] , drop_first = True)\n", "pclass = pd.get_dummies(df['Pclass'] , drop_first = True)\n", "print(df.info)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["histograms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ns.pairplot(df, hue='Pclass',diag_kind='kde')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.pairplot(df)\n", "df = pd.concat([df,sex,embarked,pclass], axis = 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.columns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.drop(['Sex','Embarked','Pclass','PassengerId'], axis = 1, inplace = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head(5)\n", "df.corr()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Multimodal Data Visualizations"]}, {"cell_type": "markdown", "metadata": {}, "source": ["correlation matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(14,12))\n", "sns.heatmap(df.corr(),linewidths=.1,cmap=\"ocean_r\", annot=True, annot_kws={\"size\": 7})\n", "plt.yticks(rotation=0)\n", "plt.savefig(\"corr.png\", format='png', dpi=900, bbox_inches='tight')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split dataset into train and test "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = df.values\n", "X = array[:,1:11]\n", "Y = array[:,0]\n", "test_size = 0.30\n", "seed = 80"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=test_size, random_state=seed)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test options and evaluation metric"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_folds = 6\n", "seed = 80\n", "scoring = 'accuracy'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Spot-Check Algorithms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = []\n", "models.append(('LR', LogisticRegression()))\n", "models.append(('LDA', LinearDiscriminantAnalysis()))\n", "models.append(('KNN', KNeighborsClassifier()))\n", "models.append(('CART', DecisionTreeClassifier()))\n", "models.append(('NB', GaussianNB()))\n", "models.append(('SVM', SVC()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = []\n", "names = []\n", "for name, model in models:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)\n", "    \n", "# Compare Algorithms\n", "fig = pyplot.figure()\n", "fig.suptitle('Algorithm Comparison')\n", "ax = fig.add_subplot(111)\n", "pyplot.boxplot(results)\n", "ax.set_xticklabels(names)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate Algorithms: Standardize/Normalize data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines<br>\n", "that standardize the data and build the # model for each fold in the cross-validation test harness.<br>\n", "That way we can get a fair estimation of how each model with standardized data might perform on unseen data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Standardize the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pipelines = []\n", "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n", "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n", "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n", "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n", "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n", "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n", "results = []\n", "names = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for name, model in pipelines:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compare Algorithms"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = pyplot.figure()\n", "fig.suptitle('Scaled Algorithm Comparison')\n", "ax = fig.add_subplot(111)\n", "pyplot.boxplot(results)\n", "ax.set_xticklabels(names)\n", "pyplot.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After investigate tuning the parameters for the algorithm that show promise from<br>\n", "In the spot-checking highest score is for : SVM.<br>\n", "Tuning SVM<br>\n", "tune two key parameters of the SVM algorithm, the value of C (how much to relax the<br>\n", "margin) and the type of kernel. The default for SVM (the SVC class) is to use the Radial<br>\n", "Basis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid<br>\n", "search using 10-fold cross-validation with a standardized copy of the training dataset. We will<br>\n", "try a number of simpler kernel types and C values with less bias and more bias (less than and<br>\n", "more than 1.0 respectively)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["une scaled SVM"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler().fit(X_train)\n", "rescaledX = scaler.transform(X_train)\n", "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n", "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n", "param_grid = dict(C=c_values, kernel=kernel_values)\n", "model = SVC()\n", "kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n", "grid_result = grid.fit(rescaledX, Y_train)\n", "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "means = grid_result.cv_results_['mean_test_score']\n", "stds = grid_result.cv_results_['std_test_score']\n", "params = grid_result.cv_results_['params']\n", "for mean, stdev, param in zip(means, stds, params):\n", "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#########Ensemble Methods###########"]}, {"cell_type": "markdown", "metadata": {}, "source": ["evaluate four different ensemble machine learning<br>\n", "algorithms, two boosting and two bagging methods:<br>\n", "1. Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).<br>\n", "2. Bagging Methods: Random Forests (RF) and Extra Trees (ET)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["ensembles"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ensembles = []\n", "ensembles.append(('AB', AdaBoostClassifier()))\n", "ensembles.append(('GBM', GradientBoostingClassifier()))\n", "ensembles.append(('RF', RandomForestClassifier()))\n", "ensembles.append(('ET', ExtraTreesClassifier()))\n", "results = []\n", "names = []\n", "for name, model in ensembles:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed,shuffle=True)\n", "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finalize model<br>\n", "SVM showed the most promise as a low complexity and stable model for this problem. In<br>\n", "so Finalize the model by training it on the entire training dataset and make<br>\n", "predictions for the hold-out validation dataset to confirm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["prepare the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler().fit(X_train)\n", "rescaledX = scaler.transform(X_train)\n", "model = SVC(C=1.5)\n", "model.fit(rescaledX, Y_train)\n", "# estimate accuracy on validation dataset\n", "rescaledValidationX = scaler.transform(X_test)\n", "predictions = model.predict(rescaledValidationX)\n", "print(accuracy_score(Y_test, predictions))\n", "print(confusion_matrix(Y_test, predictions))\n", "print(classification_report(Y_test, predictions))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}